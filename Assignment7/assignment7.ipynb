{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import nltk\n",
    "import math\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_val_predict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\bepis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "hackernews_items = pd.read_csv('hn_items.csv',delimiter=',',encoding='latin-1')\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&amp;#34;the rising star of venture capital&amp;#34; -...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there anywhere to eat on Sandhill Road?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's kind of funny that Sevin Rosen is giving ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is interesting, but the limitations becom...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stay tuned...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  neg    pos\n",
       "0  &#34;the rising star of venture capital&#34; -...  0.0  0.000\n",
       "1         Is there anywhere to eat on Sandhill Road?  0.0  0.000\n",
       "2  It's kind of funny that Sevin Rosen is giving ...  0.0  0.218\n",
       "3  This is interesting, but the limitations becom...  0.0  0.149\n",
       "4                                      Stay tuned...  0.0  0.000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "model = SentimentIntensityAnalyzer()\n",
    "\n",
    "hn_text = hackernews_items.dropna(subset=['text'], how='all')['text'].values\n",
    "\n",
    "df = pd.DataFrame(columns=['text','neg','pos'])\n",
    "\n",
    "data = []\n",
    "\n",
    "for text in hn_text:\n",
    "    score = model.polarity_scores(text)\n",
    "    \n",
    "    data.append({'text':text,'neg':score['neg'],'pos':score['pos']})\n",
    "\n",
    "df = df.append(data)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most positive: \n",
      "                text  neg  pos\n",
      "605             sure  0.0  1.0\n",
      "999              ha!  0.0  1.0\n",
      "1307      Beautiful.  0.0  1.0\n",
      "1628  Great, thanks!  0.0  1.0\n",
      "1902            True  0.0  1.0\n",
      "5 most negative: \n",
      "             text  neg  pos\n",
      "512          dupe  1.0  0.0\n",
      "1797         spam  1.0  0.0\n",
      "2962          No.  1.0  0.0\n",
      "3008       dupe.   1.0  0.0\n",
      "3014  desperation  1.0  0.0\n"
     ]
    }
   ],
   "source": [
    "print('5 most positive: ')\n",
    "print(df.nlargest(5,'pos'))\n",
    "\n",
    "print('5 most negative: ')\n",
    "print(df.nlargest(5,'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "negatives = df['neg']\n",
    "positives = df['pos']\n",
    "posts = hackernews_items.dropna(subset=['text'], how='all')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.487684198291639"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "    \n",
    "folds = KFold(n_splits=10)\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "X = negatives.values\n",
    "Y = positives.values\n",
    "XY = np.stack((X,Y),axis=1)\n",
    "\n",
    "kmeans.fit(XY)\n",
    "    \n",
    "clusters = kmeans.cluster_centers_\n",
    "    \n",
    "pred = kmeans.predict(clusters)\n",
    "\n",
    "kmeans.score(XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "boliga = pd.read_csv('boliga_zealand.csv').drop(['Index', '_1', 'Unnamed: 0'], axis=1)\n",
    "zip_df = pd.DataFrame(boliga['zip_code'].str.split(' ',1).tolist(), columns = ['zip','city'])\n",
    "boliga = boliga.assign(zip_int=zip_df['zip'])\n",
    "boliga = boliga[boliga['zip_int'].astype(int) <= 2999]\n",
    "heatmap_df = boliga[['lon','lat','price']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boliga_map = folium.Map(location=[55.676098, 12.568337], zoom_start=11)\n",
    "\n",
    "folium.Marker(location=[55.676098, 12.568337], icon=folium.Icon(color='red',icon='home')).add_to(boliga_map)\n",
    "heat_data = [(e.lat,e.lon,float(e.price)) for e in heatmap_df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<folium.plugins.heat_map.HeatMap at 0x114e2c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HeatMap(heat_data, radius=7).add_to(boliga_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Jupyter seems unable to properly load the Folium file, so it's saved as a separate file.\n",
    "boliga_map.save('heatmap.html')\n",
    "\n",
    "# If price and proximity are the only factors, then western Copenhagen seems to have a significant clustering of low-price housing.\n",
    "# Particularly, Brønshøj, Husum, Rødovre and Hvidovre have fairly low prices, along with housing along the eastern side of Kalvebod Fælled.\n",
    "# Northwestern Copenhagen is a generally low-income area, and it seems there is a clear divide in pricing in western copenhagen when moving past Frederiksberg and Valby.\n",
    "# So while pricing in these areas is low, desirability is also on the lower end. As such, somewhere in Amager south of Kastrup might be more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "\n",
    "haversine_df = boliga[['lon','lat','price','size_in_sq_m']].dropna()\n",
    "\n",
    "import math\n",
    "def haversine_distance(origin, destination):\n",
    "\n",
    "    lat_orig, lon_orig = origin\n",
    "    lat_dest, lon_dest = destination\n",
    "    radius = 6371\n",
    "\n",
    "    dlat = math.radians(lat_dest-lat_orig)\n",
    "    dlon = math.radians(lon_dest-lon_orig)\n",
    "    a = (math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat_orig)) \n",
    "        * math.cos(math.radians(lat_dest)) * math.sin(dlon / 2) * math.sin(dlon / 2))\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>price</th>\n",
       "      <th>size_in_sq_m</th>\n",
       "      <th>km_to_cph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.593629</td>\n",
       "      <td>55.671769</td>\n",
       "      <td>4000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.657329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.573689</td>\n",
       "      <td>55.676839</td>\n",
       "      <td>4895000</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.345533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.590441</td>\n",
       "      <td>55.687079</td>\n",
       "      <td>250000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.846924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.591287</td>\n",
       "      <td>55.683439</td>\n",
       "      <td>7375000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.654270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.588744</td>\n",
       "      <td>55.687623</td>\n",
       "      <td>5825000</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.810790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lon        lat    price  size_in_sq_m  km_to_cph\n",
       "0  12.593629  55.671769  4000000          91.0   1.657329\n",
       "1  12.573689  55.676839  4895000         105.0   0.345533\n",
       "2  12.590441  55.687079   250000         135.0   1.846924\n",
       "3  12.591287  55.683439  7375000          98.0   1.654270\n",
       "4  12.588744  55.687623  5825000         101.0   1.810790"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph_center = (55.676098, 12.568337)\n",
    "\n",
    "haversine_df = haversine_df.assign(km_to_cph=haversine_df.apply(lambda row: haversine_distance((row['lat'],row['lon']),cph_center),axis=1))\n",
    "haversine_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "X_variables = haversine_df['km_to_cph'].astype(float).values.reshape(-1,1)\n",
    "Y_variables = haversine_df['price'].astype(int).values.reshape(-1,1)\n",
    "Z_variables = haversine_df['size_in_sq_m'].astype(float).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Coefficients</th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1174037.38883</td>\n",
       "      <td>2144552.4767868356</td>\n",
       "      <td>0.211113686177</td>\n",
       "      <td>[[-50144.1468458   21173.90529618]]</td>\n",
       "      <td>[ 498250.87960243]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100120.04932</td>\n",
       "      <td>1918685.8483756061</td>\n",
       "      <td>0.237796142476</td>\n",
       "      <td>[[-54502.22830107  21614.21966869]]</td>\n",
       "      <td>[ 512827.04731026]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1036518.49321</td>\n",
       "      <td>1878553.5156161697</td>\n",
       "      <td>0.25252708392</td>\n",
       "      <td>[[-54640.54807139  21551.22354509]]</td>\n",
       "      <td>[ 519284.99086517]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766123.214978</td>\n",
       "      <td>1293033.7661722663</td>\n",
       "      <td>0.328076739947</td>\n",
       "      <td>[[-55365.40297733  21340.49817169]]</td>\n",
       "      <td>[ 559535.83784652]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>769638.417343</td>\n",
       "      <td>1249136.3929791877</td>\n",
       "      <td>0.355233416161</td>\n",
       "      <td>[[-55421.10077292  21330.05818676]]</td>\n",
       "      <td>[ 564876.98863496]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>843071.068122</td>\n",
       "      <td>1579055.9428374353</td>\n",
       "      <td>0.0670818180413</td>\n",
       "      <td>[[-57500.9406537   21894.24749629]]</td>\n",
       "      <td>[ 557734.33834066]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>886109.701979</td>\n",
       "      <td>1388383.0180807484</td>\n",
       "      <td>-0.289723668799</td>\n",
       "      <td>[[-51126.71813158  22408.71421293]]</td>\n",
       "      <td>[ 475862.06201267]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>989510.690701</td>\n",
       "      <td>1973721.2285719078</td>\n",
       "      <td>-0.0814055288237</td>\n",
       "      <td>[[-56006.40772878  22601.03956962]]</td>\n",
       "      <td>[ 464041.60604401]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1149840.48351</td>\n",
       "      <td>1694459.9511825275</td>\n",
       "      <td>0.22881455553</td>\n",
       "      <td>[[-59155.94246881  21941.8961982 ]]</td>\n",
       "      <td>[ 525654.91765968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1611217.43888</td>\n",
       "      <td>3187611.1353421817</td>\n",
       "      <td>0.178544735339</td>\n",
       "      <td>[[-66764.29094814  19669.18416474]]</td>\n",
       "      <td>[ 737235.23392789]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MAE                RMSE           Pearson  \\\n",
       "0  1174037.38883  2144552.4767868356    0.211113686177   \n",
       "1  1100120.04932  1918685.8483756061    0.237796142476   \n",
       "2  1036518.49321  1878553.5156161697     0.25252708392   \n",
       "3  766123.214978  1293033.7661722663    0.328076739947   \n",
       "4  769638.417343  1249136.3929791877    0.355233416161   \n",
       "5  843071.068122  1579055.9428374353   0.0670818180413   \n",
       "6  886109.701979  1388383.0180807484   -0.289723668799   \n",
       "7  989510.690701  1973721.2285719078  -0.0814055288237   \n",
       "8  1149840.48351  1694459.9511825275     0.22881455553   \n",
       "9  1611217.43888  3187611.1353421817    0.178544735339   \n",
       "\n",
       "                          Coefficients           Intercept  \n",
       "0  [[-50144.1468458   21173.90529618]]  [ 498250.87960243]  \n",
       "1  [[-54502.22830107  21614.21966869]]  [ 512827.04731026]  \n",
       "2  [[-54640.54807139  21551.22354509]]  [ 519284.99086517]  \n",
       "3  [[-55365.40297733  21340.49817169]]  [ 559535.83784652]  \n",
       "4  [[-55421.10077292  21330.05818676]]  [ 564876.98863496]  \n",
       "5  [[-57500.9406537   21894.24749629]]  [ 557734.33834066]  \n",
       "6  [[-51126.71813158  22408.71421293]]  [ 475862.06201267]  \n",
       "7  [[-56006.40772878  22601.03956962]]  [ 464041.60604401]  \n",
       "8  [[-59155.94246881  21941.8961982 ]]  [ 525654.91765968]  \n",
       "9  [[-66764.29094814  19669.18416474]]  [ 737235.23392789]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "folds = KFold(n_splits=10)\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['MAE','RMSE','Pearson','Coefficients','Intercept'])\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for train_idx, test_idx in folds.split(X_variables, Y_variables, Z_variables):\n",
    "    XZ_variables = np.stack([X_variables,Z_variables], axis=1).reshape(-1,2)\n",
    "    XZ_train, XZ_test = XZ_variables[train_idx], XZ_variables[test_idx]\n",
    "    Y_train, Y_test = Y_variables[train_idx], Y_variables[test_idx]\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    \n",
    "    regr.fit(XZ_train, Y_train)\n",
    "    \n",
    "    pred = regr.predict(XZ_test)\n",
    "    \n",
    "    MAE = str(metrics.mean_absolute_error(Y_test,pred))\n",
    "    RMSE = str(sqrt(metrics.mean_squared_error(Y_test,pred)))\n",
    "    Pearson = str(metrics.r2_score(Y_test, pred))\n",
    "    coef = str(regr.coef_)\n",
    "    intercept = str(regr.intercept_)\n",
    "    \n",
    "    metrics_list.append({'MAE':MAE,'RMSE':RMSE,'Pearson':Pearson,'Coefficients':coef,'Intercept':intercept})\n",
    "\n",
    "metrics_df = metrics_df.append(metrics_list)\n",
    "    \n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As is plainly clear, the model can't help us untuit anything - in some cases it might actually be worse to follow the model than not.\n",
    "# Size and proximity to the city center are not exactly fully representative - the price variance between several homes of equal size is too huge to be a reliable metric.\n",
    "# Bispebjerg and Amager Øst are of approximately equal distance to Nørreport, where the former is notoriously low-income, while the latter is notoriously high-income.\n",
    "# Also, prices have changed drastically over time, as gentrification of certain parts of the city has greatly increased prices of homes in recent years.\n",
    "# We would need both a greater dimensionality, as well as data more representative of current prices to accurately intuit prices of homes in the city."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
